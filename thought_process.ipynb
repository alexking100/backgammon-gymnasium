{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning for Backgammon: Thought Process & Exploration\n",
        "\n",
        "This notebook walks through the thought process of applying RL to backgammon, exploring the environment, and understanding why certain approaches work better than others.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Game Setup](#1-game-setup)\n",
        "2. [Understanding the State Space](#2-understanding-the-state-space)\n",
        "3. [Understanding the Action Space](#3-understanding-the-action-space)\n",
        "4. [Q-Learning](#4-q-learning)\n",
        "5. [Deep Q-Network](#5-deep-q-network)\n",
        "6. [Value Function Approximation](#6-value-function-approximation)\n",
        "7. [Model Comparisons](#7-model-comparisons)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Game Setup\n",
        "\n",
        "Link to the rules of BackGammon: https://www.bkgm.com/rules.html\n",
        "\n",
        "We define the game infrastructure in gym_backgammon/envs/backgammon.py and the gym environment is created for ease of RL implementation in gym_backgammon/envs/backgammon_env.py\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding the State Space\n",
        "\n",
        "The way we represent the game is as follows: for each player, we have\n",
        "- 96-vector containing \n",
        "    - 4 elements for each of the 24 slots on the backgammon board. \n",
        "    - The 4 elements represent the number of checkers on each slot.\n",
        "    - 4-dim Feature encoding: [has_1_checker, has_2_checkers, has_3+_checkers, (count-3)/2]\n",
        "\n",
        "- 2-vector to store (checkers on the bar, checkers off the board)\n",
        "\n",
        "then finally a 2-vector is appended to note whose turn it is. The total dimensionality is therefore 192 + 2 + 2 + 2 = 198. This is the State Space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding the Action Space\n",
        "\n",
        "An action is represented like ((source1, target1), (source2, target2), ...) which describes a valid combination of checker-moves. The action is chosen from a set of valid actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Q-Learning\n",
        "\n",
        "Populate Q-value table such that for every board state there is an optimal action with the highest Q-Value. If we had this table, we could trivially look up what the most optimal action is. The highest Q-value is the most likely to bring us to the reward.\n",
        "\n",
        "To generate this Q-value table we do the following \n",
        "\n",
        "$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha (r + \\gamma (\\text{max}_iQ(s_{+1}, a_i) - Q(s, a)) $$\n",
        "\n",
        "where $r$ is reward (so in the case of backgammon, 1 when the game is won and 0 otherwise), $\\alpha$ is some learning rate, and $\\gamma$ is some discounting factor: if we set $\\gamma=1$, we care equally about future rewards as we do current rewards. If we set $\\gamma=0$, we care only about immediate rewards and shall only update Q value if we see an increased reward.\n",
        "\n",
        "There are a couple of problems here for the Q-Learning approach\n",
        "1. The size of states is $\\sim 10^{20}$, way too large for any table to store values\n",
        "2. There is only non-zero reward once a player has won the game. This means the iteration on Q values is slow and unstable, even with extreme $\\alpha$ and $\\gamma$.\n",
        "\n",
        "<span style=\"color:lightcoral\">\n",
        "‚ùì Is it possible to do Q-Learning at all? Can we try DQN \n",
        "</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deep Q-Network\n",
        "\n",
        "This approximates the Q-value as a neural network instead of a look-up table. In theory this could work here, but still problem 2 from above still holds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Value Function Approximation\n",
        "\n",
        "Instead of approximating a Q value, which requires (state, action) as input, we can use a neural network to approximate a value function, which estimates the probability of reward at the current state.\n",
        "\n",
        "$$ V(s_t) \\leftarrow V(s_t) + \\alpha(r_{t+1} + \\gamma (V(s_{t+1}) - V(s_t)) )$$\n",
        "\n",
        "where we call the right hand part $\\delta_t=r_{t+1} + \\gamma (V(s_{t+1}) - V(s_t))$.\n",
        "\n",
        "So how does the model actually learn? \n",
        "\n",
        "1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
